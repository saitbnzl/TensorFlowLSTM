{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow for RNN and Numpy to prepare our own data.<a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (2000, 10)\n",
      "first element: [0 1 0 1 0 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 2000\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "sequences = np.zeros((NUM_EXAMPLES, SEQ_LENGTH), dtype=np.int8)\n",
    "# How many number of ones in each sequence\n",
    "number_of_1s = np.random.randint(0, SEQ_LENGTH, size=NUM_EXAMPLES)\n",
    "\n",
    "indices = np.arange(SEQ_LENGTH)\n",
    "for idx, num_ones in enumerate(number_of_1s.tolist()):\n",
    "    # Set \"num_ones\" elements to 1 using \"choice\" without replace.\n",
    "    sequences[idx][np.random.choice(indices, num_ones, replace=False)] = 1\n",
    "\n",
    "print(\"shape of input data:\",sequences.shape)\n",
    "print(\"first element:\", sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify sequences with size 10. For this purpose we're creating a dataset that includes 2.000 example array. Each array consists of 0's and 1's. Number of 1's and 0's is random. Number of 1's determines the array's class. So we can only have 11 classes at most because our array length is 10. (Don't forget we can have all zeroes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a bit complex to generate random arrays. I will explain [the reason](#thereason) later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 2, ..., 8, 0, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = []\n",
    "for input in sequences: \n",
    "    target = (input == 1).sum()\n",
    "    target_classes.append(target)\n",
    "\n",
    "target_classes = np.asarray(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate quantites of each classes and append them to **target_classes**.<a name=\"thereason\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93f8be3780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target_classes,bins=np.arange(SEQ_LENGTH+1),histtype='step')\n",
    "plt.xticks(np.arange(SEQ_LENGTH+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the histogram graph of our classes. You can see the quantity of examples(y-axis) are very close among the classes. We ensured that our data to be evenly distributed among the classes with our array generation [implementation](#generation) above. Because this is generally better for ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a supervised learning method we need to know the answers. This means we need to have the correct classes of our sequences for training. So we count the number of 1's for each array and we append them to a 1D array.\n",
    "\n",
    "Now we need to encode our label array with 1-hot encoding. Because in Machine Learning algorithms we tend to encode our class labels with 1-hot encoding. There are a couple of reasons for this. For example, in this problem our network can predict the class labels with probabilities instead of exact class labels. Like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.23019266,  0.19126155,  0.07210049,  0.06715287,  0.02359275,\n",
       "        0.04310431,  0.12523196,  0.08166963,  0.11487669,  0.05081709])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.exponential(2,10)\n",
    "sample /= sample.sum()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see why 1-hot encoding is useful in this case. You can think like that:\n",
    "\"With 1-hot encoding we say:\n",
    "\"This is an apple 100% and this is a banana 0%\", instead of saying just \"This is an apple\". Now let's see how we can encode our label array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is just creating a unit matrix with the size as a parameter. But if you look carefully this is 1-hot encoded array between 0 and 10. This is actually our 1-hot encoded class labels. We just need to encode our training data using this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = np.eye(11)[target_classes]\n",
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is why Python is awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1500, 10), (1, 400, 10), (1, 100, 10), (1500, 11), (400, 11), (100, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trainx = 1500\n",
    "n_validx = 400\n",
    "trainX  = sequences[:n_trainx]\n",
    "trainY = target_classes[:n_trainx]\n",
    "validX = sequences[n_trainx:n_trainx+n_validx]\n",
    "validY = target_classes[n_trainx:n_trainx+n_validx]\n",
    "testX = sequences[n_trainx+n_validx:]\n",
    "testY = target_classes[n_trainx+n_validx:]\n",
    "\n",
    "trainX = trainX.reshape(1,trainX.shape[0],trainX.shape[1])\n",
    "validX = validX.reshape(1,validX.shape[0],validX.shape[1])\n",
    "testX = testX.reshape(1,testX.shape[0],testX.shape[1])\n",
    "\n",
    "trainX.shape,validX.shape,testX.shape,trainY.shape,validY.shape,testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've splitted our data in two parts. 1.000 of them is for training, 400 for validation and remaining 100 is for testing. We've reshaped our input data in a 3D shape. Because that's what TensorFlow RNN function's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 10\n",
    "n_chunks = 28\n",
    "x = tf.placeholder(\"float32\", [None, None, SEQ_LENGTH])\n",
    "y = tf.placeholder(\"int32\", [None, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two placeholders. x for input y for class labels. 'n_hidden' means \"number of hidden layers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_index):\n",
    "        # Go to the next epoch\n",
    "        if batch_index*batch_size + batch_size > trainX.shape[1]:\n",
    "            # Finished epoch\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = trainX.shape[1] - batch_index*batch_size\n",
    "            input_rest_part = trainX[:,trainX.shape[1]-rest_num_examples:trainX.shape[1]]\n",
    "            labels_rest_part = trainY[trainY.shape[0]-rest_num_examples:trainY.shape[0]]\n",
    "            # Start next epoch\n",
    "            batch_index = 0\n",
    "            start = batch_index * batch_size\n",
    "            end = start + batch_size\n",
    "            input_new_part = trainX[:,start:end]\n",
    "            labels_new_part = trainY[start:end]\n",
    "            batch_index += 1\n",
    "            return np.concatenate((input_rest_part, input_new_part), axis=1), np.concatenate(\n",
    "                (labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            start = batch_index*batch_size\n",
    "            end = start+batch_size\n",
    "            batch_index += 1\n",
    "            return trainX[:,start:end], trainY[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits the training data to batches in batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_hidden, 11]))\n",
    "biases = tf.Variable(tf.random_normal([11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our weight and bias variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden,state_is_tuple=True)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'output[-1]' means the last output in a an array of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x):\n",
    "    pred = RNN(x,weights,biases)\n",
    "    output = pred\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    index_of_max_prob = tf.argmax(softmax, 1)\n",
    "    correct_labels =  tf.argmax(y, 1)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 240\n",
    "    display_step = 20\n",
    "    with tf.variable_scope('training'):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Before training|Prediction for first 20 sequences:\",index_of_max_prob.eval({x:testX[:,0:20]}))\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch_index in range(int(n_trainx/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(batch_index)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                if (epoch)%display_step==0 or (epoch+1) == hm_epochs:\n",
    "                    print('Epoch', (epoch), 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                    print('Training accuracy:',accuracy.eval({x:trainX, y:trainY}))\n",
    "                    print('Validation accuracy:',accuracy.eval({x:validX, y:validY}))\n",
    "                    print('Test accuracy:',accuracy.eval({x:testX, y:testY}))\n",
    "                    print(\"--------------------------------------------------------\")\n",
    "            print(\"Optimization finished!\") \n",
    "            print('%50s  %50s' % (\"After training|Prediction for first 20 sequences:\",index_of_max_prob.eval({x:testX[:,0:20]})))\n",
    "            print('%50s  %50s' % (\"Correct labels for first 20 sequences:\",correct_labels.eval({y:testY[:20]})))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our network and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training|Prediction for first 20 sequences: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 4]\n",
      "Epoch 0 completed out of 240 loss: 43.5364196301\n",
      "Training accuracy: 0.116\n",
      "Validation accuracy: 0.0725\n",
      "Test accuracy: 0.14\n",
      "--------------------------------------------------------\n",
      "Epoch 20 completed out of 240 loss: 29.2086699009\n",
      "Training accuracy: 0.209333\n",
      "Validation accuracy: 0.2075\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------\n",
      "Epoch 40 completed out of 240 loss: 24.430064559\n",
      "Training accuracy: 0.426667\n",
      "Validation accuracy: 0.365\n",
      "Test accuracy: 0.37\n",
      "--------------------------------------------------------\n",
      "Epoch 60 completed out of 240 loss: 21.2379722595\n",
      "Training accuracy: 0.591333\n",
      "Validation accuracy: 0.5575\n",
      "Test accuracy: 0.54\n",
      "--------------------------------------------------------\n",
      "Epoch 80 completed out of 240 loss: 18.8037056923\n",
      "Training accuracy: 0.682\n",
      "Validation accuracy: 0.6425\n",
      "Test accuracy: 0.65\n",
      "--------------------------------------------------------\n",
      "Epoch 100 completed out of 240 loss: 17.0516725779\n",
      "Training accuracy: 0.776667\n",
      "Validation accuracy: 0.74\n",
      "Test accuracy: 0.77\n",
      "--------------------------------------------------------\n",
      "Epoch 120 completed out of 240 loss: 15.6623809338\n",
      "Training accuracy: 0.844\n",
      "Validation accuracy: 0.815\n",
      "Test accuracy: 0.9\n",
      "--------------------------------------------------------\n",
      "Epoch 140 completed out of 240 loss: 14.484821856\n",
      "Training accuracy: 0.886667\n",
      "Validation accuracy: 0.8525\n",
      "Test accuracy: 0.92\n",
      "--------------------------------------------------------\n",
      "Epoch 160 completed out of 240 loss: 13.4435659647\n",
      "Training accuracy: 0.913333\n",
      "Validation accuracy: 0.8775\n",
      "Test accuracy: 0.93\n",
      "--------------------------------------------------------\n",
      "Epoch 180 completed out of 240 loss: 12.5030872226\n",
      "Training accuracy: 0.920667\n",
      "Validation accuracy: 0.8925\n",
      "Test accuracy: 0.94\n",
      "--------------------------------------------------------\n",
      "Epoch 200 completed out of 240 loss: 11.637975812\n",
      "Training accuracy: 0.924667\n",
      "Validation accuracy: 0.9075\n",
      "Test accuracy: 0.97\n",
      "--------------------------------------------------------\n",
      "Epoch 220 completed out of 240 loss: 10.8066062927\n",
      "Training accuracy: 0.93\n",
      "Validation accuracy: 0.9175\n",
      "Test accuracy: 0.95\n",
      "--------------------------------------------------------\n",
      "Epoch 239 completed out of 240 loss: 10.0900301933\n",
      "Training accuracy: 0.934\n",
      "Validation accuracy: 0.9175\n",
      "Test accuracy: 0.93\n",
      "--------------------------------------------------------\n",
      "Optimization finished!\n",
      " After training|Prediction for first 20 sequences:           [9 8 5 0 0 8 3 2 3 9 0 7 5 4 4 2 5 7 4 3]\n",
      "            Correct labels for first 20 sequences:           [9 5 5 0 0 8 3 2 3 9 0 6 5 4 4 2 5 7 4 3]\n"
     ]
    }
   ],
   "source": [
    "train(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
