{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow for RNN and Numpy to prepare our own data.<a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (2000, 10)\n",
      "first element: [1 1 1 1 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 2000\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "sequences = np.zeros((NUM_EXAMPLES, SEQ_LENGTH), dtype=np.int8)\n",
    "# How many number of ones in each sequence\n",
    "number_of_1s = np.random.randint(0, SEQ_LENGTH, size=NUM_EXAMPLES)\n",
    "\n",
    "indices = np.arange(SEQ_LENGTH)\n",
    "for idx, num_ones in enumerate(number_of_1s.tolist()):\n",
    "    # Set \"num_ones\" elements to 1 using \"choice\" without replace.\n",
    "    sequences[idx][np.random.choice(indices, num_ones, replace=False)] = 1\n",
    "\n",
    "print(\"shape of input data:\",sequences.shape)\n",
    "print(\"first element:\", sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a bit complex to generate random arrays. I will explain [the reason](#thereason) later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify sequences with size 10. For this purpose we're creating a dataset that includes 2.000 example array. Each array consists of 0's and 1's. Number of 1's and 0's is random. Number of 1's determines the array's class. So we can only have 11 classes at most because our array length is 10. (Don't forget we can have all zeroes) We will use a very powerful method for a simple job to understand the basics of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 6, 9, ..., 0, 7, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = []\n",
    "for input in sequences: \n",
    "    target = (input == 1).sum()\n",
    "    target_classes.append(target)\n",
    "\n",
    "target_classes = np.asarray(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate quantites of each classes and append them to **target_classes**.<a name=\"thereason\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4d1aca3a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target_classes,bins=np.arange(SEQ_LENGTH+1),histtype='step')\n",
    "plt.xticks(np.arange(SEQ_LENGTH+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the histogram graph of our classes. You can see the quantity of examples(y-axis) are very close among the classes. We ensured that our data to be evenly distributed among the classes with our array generation [implementation](#generation) above. Because this is generally better for ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a supervised learning method we need to know the answers. This means we need to have the correct classes of our sequences for training. So we count the number of 1's for each array and we append them to a 1D array. Now let's encode our training data with 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is just creating a unit matrix with the size as a parameter. But if you look carefully this is 1-hot encoded array between 0 and 10. This is actually our 1-hot encoded class labels. We just need to encode our training data using this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = np.eye(11)[target_classes]\n",
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is why Python is awesome. We tend to represent our class labels with 1-hot encoding in ML methods. There are a couple of reasons for this. For example, our network can make predictions with probabilities instead of the exact labels like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04446441,  0.22966688,  0.01996275,  0.12006995,  0.02014695,\n",
       "        0.07939978,  0.15518785,  0.01036897,  0.05463017,  0.26610229])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.exponential(2,10)\n",
    "sample /= sample.sum()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see why 1-hot encoding is useful in this case. You can think like that:\n",
    "\"With 1-hot encoding we say:\n",
    "\"This is an apple 100% and this is a banana 0%\", instead of saying just \"This is an apple\". Now let's see how we can encode our label array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1500, 10), (1, 400, 10), (1, 100, 10), (1500, 11), (400, 11), (100, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trainx = 1500\n",
    "n_validx = 400\n",
    "trainX  = sequences[:n_trainx]\n",
    "trainY = target_classes[:n_trainx]\n",
    "validX = sequences[n_trainx:n_trainx+n_validx]\n",
    "validY = target_classes[n_trainx:n_trainx+n_validx]\n",
    "testX = sequences[n_trainx+n_validx:]\n",
    "testY = target_classes[n_trainx+n_validx:]\n",
    "\n",
    "trainX = trainX.reshape(1,trainX.shape[0],trainX.shape[1])\n",
    "validX = validX.reshape(1,validX.shape[0],validX.shape[1])\n",
    "testX = testX.reshape(1,testX.shape[0],testX.shape[1])\n",
    "\n",
    "trainX.shape,validX.shape,testX.shape,trainY.shape,validY.shape,testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've splitted our data in two parts. 1.000 of them is for training, 400 for validation and remaining 100 is for testing. We've reshaped our input data in a 3D shape. Because that's what TensorFlow RNN function's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_index):\n",
    "        # Go to the next epoch\n",
    "        if batch_index*batch_size + batch_size > trainX.shape[1]:\n",
    "            # Finished epoch\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = trainX.shape[1] - batch_index*batch_size\n",
    "            input_rest_part = trainX[:,trainX.shape[1]-rest_num_examples:trainX.shape[1]]\n",
    "            labels_rest_part = trainY[trainY.shape[0]-rest_num_examples:trainY.shape[0]]\n",
    "            # Start next epoch\n",
    "            batch_index = 0\n",
    "            start = batch_index * batch_size\n",
    "            end = start + batch_size\n",
    "            input_new_part = trainX[:,start:end]\n",
    "            labels_new_part = trainY[start:end]\n",
    "            batch_index += 1\n",
    "            return np.concatenate((input_rest_part, input_new_part), axis=1), np.concatenate(\n",
    "                (labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            start = batch_index*batch_size\n",
    "            end = start+batch_size\n",
    "            batch_index += 1\n",
    "            return trainX[:,start:end], trainY[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits the training data to batches in batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 10\n",
    "x = tf.placeholder(\"float32\", [None, None, SEQ_LENGTH])\n",
    "y = tf.placeholder(\"int32\", [None, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two placeholders for hour model. x for input y for class labels. 'n_hidden' means \"number of hidden layers\" or \"number of units in our LSTM cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_hidden, 11]))\n",
    "biases = tf.Variable(tf.random_normal([11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden,state_is_tuple=True)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our weight and bias variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'output[-1]' means the last output in a an array of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x):\n",
    "    pred = RNN(x,weights,biases)\n",
    "    output = pred\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    index_of_max_prob = tf.argmax(softmax, 1)\n",
    "    correct_labels =  tf.argmax(y, 1)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 240\n",
    "    display_step = 20\n",
    "    with tf.variable_scope('training'):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Before training|Prediction for first 10 sequences:\",index_of_max_prob.eval({x:testX[:,0:10]}))\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch_index in range(int(n_trainx/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(batch_index)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                if (epoch)%display_step==0 or (epoch+1) == hm_epochs:\n",
    "                    print('Epoch', (epoch), 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                    print('Training accuracy:',accuracy.eval({x:trainX, y:trainY}))\n",
    "                    print('Validation accuracy:',accuracy.eval({x:validX, y:validY}))\n",
    "                    print('Test accuracy:',accuracy.eval({x:testX, y:testY}))\n",
    "                    print(\"--------------------------------------------------------\")\n",
    "            print(\"Optimization finished!\") \n",
    "            print('%50s  %50s' % (\"After training|Prediction for first 10 sequences:\",index_of_max_prob.eval({x:testX[:,0:10]})))\n",
    "            print('%50s  %50s' % (\"Correct labels for first 10 sequences:\",correct_labels.eval({y:testY[:10]})))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our network and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training|Prediction for first 10 sequences: [8 8 8 8 6 6 6 6 6 8]\n",
      "Epoch 0 completed out of 240 loss: 44.4572672844\n",
      "Training accuracy: 0.094\n",
      "Validation accuracy: 0.0625\n",
      "Test accuracy: 0.08\n",
      "--------------------------------------------------------\n",
      "Epoch 20 completed out of 240 loss: 29.0918596983\n",
      "Training accuracy: 0.338667\n",
      "Validation accuracy: 0.3075\n",
      "Test accuracy: 0.26\n",
      "--------------------------------------------------------\n",
      "Epoch 40 completed out of 240 loss: 23.2442053556\n",
      "Training accuracy: 0.528\n",
      "Validation accuracy: 0.5\n",
      "Test accuracy: 0.43\n",
      "--------------------------------------------------------\n",
      "Epoch 60 completed out of 240 loss: 19.8891512156\n",
      "Training accuracy: 0.641333\n",
      "Validation accuracy: 0.6025\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------\n",
      "Epoch 80 completed out of 240 loss: 17.4604520798\n",
      "Training accuracy: 0.732\n",
      "Validation accuracy: 0.73\n",
      "Test accuracy: 0.65\n",
      "--------------------------------------------------------\n",
      "Epoch 100 completed out of 240 loss: 15.5985856056\n",
      "Training accuracy: 0.816\n",
      "Validation accuracy: 0.81\n",
      "Test accuracy: 0.7\n",
      "--------------------------------------------------------\n",
      "Epoch 120 completed out of 240 loss: 14.1235606074\n",
      "Training accuracy: 0.885333\n",
      "Validation accuracy: 0.8675\n",
      "Test accuracy: 0.83\n",
      "--------------------------------------------------------\n",
      "Epoch 140 completed out of 240 loss: 12.9106912613\n",
      "Training accuracy: 0.92\n",
      "Validation accuracy: 0.9075\n",
      "Test accuracy: 0.86\n",
      "--------------------------------------------------------\n",
      "Epoch 160 completed out of 240 loss: 11.8985680938\n",
      "Training accuracy: 0.941333\n",
      "Validation accuracy: 0.9325\n",
      "Test accuracy: 0.87\n",
      "--------------------------------------------------------\n",
      "Epoch 180 completed out of 240 loss: 11.0018109679\n",
      "Training accuracy: 0.952667\n",
      "Validation accuracy: 0.9425\n",
      "Test accuracy: 0.89\n",
      "--------------------------------------------------------\n",
      "Epoch 200 completed out of 240 loss: 10.1828347445\n",
      "Training accuracy: 0.962\n",
      "Validation accuracy: 0.945\n",
      "Test accuracy: 0.89\n",
      "--------------------------------------------------------\n",
      "Epoch 220 completed out of 240 loss: 9.45067304373\n",
      "Training accuracy: 0.969333\n",
      "Validation accuracy: 0.95\n",
      "Test accuracy: 0.9\n",
      "--------------------------------------------------------\n",
      "Epoch 239 completed out of 240 loss: 8.82099533081\n",
      "Training accuracy: 0.974\n",
      "Validation accuracy: 0.95\n",
      "Test accuracy: 0.93\n",
      "--------------------------------------------------------\n",
      "Optimization finished!\n",
      " After training|Prediction for first 10 sequences:                               [3 6 9 8 9 1 1 8 0 4]\n",
      "            Correct labels for first 10 sequences:                               [1 3 9 7 8 1 1 8 0 4]\n"
     ]
    }
   ],
   "source": [
    "train(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
