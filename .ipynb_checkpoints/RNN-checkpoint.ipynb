{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow for RNN and Numpy to prepare our own data.<a name=\"generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (2000, 10)\n",
      "first element: [1 0 0 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 2000\n",
    "SEQ_LENGTH = 10\n",
    "\n",
    "sequences = np.zeros((NUM_EXAMPLES, SEQ_LENGTH), dtype=np.int8)\n",
    "# How many number of ones in each sequence\n",
    "number_of_1s = np.random.randint(0, SEQ_LENGTH, size=NUM_EXAMPLES)\n",
    "\n",
    "indices = np.arange(SEQ_LENGTH)\n",
    "for idx, num_ones in enumerate(number_of_1s.tolist()):\n",
    "    # Set \"num_ones\" elements to 1 using \"choice\" without replace.\n",
    "    sequences[idx][np.random.choice(indices, num_ones, replace=False)] = 1\n",
    "\n",
    "print(\"shape of input data:\",sequences.shape)\n",
    "print(\"first element:\", sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is a bit complex to generate random arrays. I will explain [the reason](#thereason) later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify sequences with size 10. For this purpose we're creating a dataset that includes 2.000 example array. Each array consists of 0's and 1's. Number of 1's and 0's is random. Number of 1's determines the array's class. So we can only have 11 classes at most because our array length is 10. (Don't forget we can have all zeroes) We will use a very powerful method for a simple job to understand the basics of the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 6, 9, ..., 8, 7, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = []\n",
    "for input in sequences: \n",
    "    target = (input == 1).sum()\n",
    "    target_classes.append(target)\n",
    "\n",
    "target_classes = np.asarray(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate quantites of each classes and append them to **target_classes**.<a name=\"thereason\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+V5lrM8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe12bc63b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(target_classes,bins=np.arange(SEQ_LENGTH+1),histtype='step')\n",
    "plt.xticks(np.arange(SEQ_LENGTH+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the histogram graph of our classes. You can see the quantity of examples(y-axis) are very close among the classes. We ensured that our data to be evenly distributed among the classes with our array generation [implementation](#generation) above. Because this is generally better for ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a supervised learning method we need to know the answers. This means we need to have the correct classes of our sequences for training. So we count the number of 1's for each array and we append them to a 1D array. Now let's encode our training data with 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is just creating a unit matrix with the size as a parameter. But if you look carefully this is 1-hot encoded array between 0 and 10. This is actually our 1-hot encoded class labels. We just need to encode our training data using this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = np.eye(11)[target_classes]\n",
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is why Python is awesome. We tend to represent our class labels with 1-hot encoding in ML methods. There are a couple of reasons for this. For example, our network can make predictions with probabilities instead of the exact labels like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14325605,  0.03098785,  0.10402645,  0.03745838,  0.10591188,\n",
       "        0.17123628,  0.13545909,  0.01604602,  0.17634159,  0.07927642])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.exponential(2,10)\n",
    "sample /= sample.sum()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see why 1-hot encoding is useful in this case. You can think like that:\n",
    "\"With 1-hot encoding we say:\n",
    "\"This is an apple 100% and this is a banana 0%\", instead of saying just \"This is an apple\". Now let's see how we can encode our label array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1500, 10), (1, 400, 10), (1, 100, 10), (1500, 11), (400, 11), (100, 11))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_trainx = 1500\n",
    "n_validx = 400\n",
    "trainX  = sequences[:n_trainx]\n",
    "trainY = target_classes[:n_trainx]\n",
    "validX = sequences[n_trainx:n_trainx+n_validx]\n",
    "validY = target_classes[n_trainx:n_trainx+n_validx]\n",
    "testX = sequences[n_trainx+n_validx:]\n",
    "testY = target_classes[n_trainx+n_validx:]\n",
    "\n",
    "trainX = trainX.reshape(1,trainX.shape[0],trainX.shape[1])\n",
    "validX = validX.reshape(1,validX.shape[0],validX.shape[1])\n",
    "testX = testX.reshape(1,testX.shape[0],testX.shape[1])\n",
    "\n",
    "trainX.shape,validX.shape,testX.shape,trainY.shape,validY.shape,testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've splitted our data in two parts. 1.000 of them is for training, 400 for validation and remaining 100 is for testing. We've reshaped our input data in a 3D shape. Because that's what TensorFlow RNN function's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_index):\n",
    "        # Go to the next epoch\n",
    "        if batch_index*batch_size + batch_size > trainX.shape[1]:\n",
    "            # Finished epoch\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = trainX.shape[1] - batch_index*batch_size\n",
    "            input_rest_part = trainX[:,trainX.shape[1]-rest_num_examples:trainX.shape[1]]\n",
    "            labels_rest_part = trainY[trainY.shape[0]-rest_num_examples:trainY.shape[0]]\n",
    "            # Start next epoch\n",
    "            batch_index = 0\n",
    "            start = batch_index * batch_size\n",
    "            end = start + batch_size\n",
    "            input_new_part = trainX[:,start:end]\n",
    "            labels_new_part = trainY[start:end]\n",
    "            batch_index += 1\n",
    "            return np.concatenate((input_rest_part, input_new_part), axis=1), np.concatenate(\n",
    "                (labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            start = batch_index*batch_size\n",
    "            end = start+batch_size\n",
    "            batch_index += 1\n",
    "            return trainX[:,start:end], trainY[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits the training data to batches in batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 10\n",
    "x = tf.placeholder(\"float32\", [None, None, SEQ_LENGTH])\n",
    "y = tf.placeholder(\"int32\", [None, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two placeholders for hour model. x for input y for class labels. 'n_hidden' means \"number of hidden layers\" or \"number of units in our LSTM cell. At a very high level we can say this number determines the size of working memory of our LSTM cell. Few number of units may cause bad accuracy, too high number may cause overfitting. Tuning hyperparametes like this is more of an art part of Neural Networks. Below picture represents a single LSTM unit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LSTM_unit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "It takes an input $x_t$, state of the previous unit $h_{t-1}$ and outputs a hidden state $h_t$ and an output. There are variatons of LSTM units. I won't go into details. The thing is we connect these units each other like below:\n",
    "<img src=\"images/chain.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we say \"number of hidden units\" we're talking about the number these units. This is one of the most important thing that specifies the learning capacity of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_hidden, 11]))\n",
    "biases = tf.Variable(tf.random_normal([11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our RNN cell finish its job and output a value we will multiply this output with a weight matrix and add a bias like we do in a simple feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    cell = tf.nn.rnn_cell.LSTMCell(n_hidden,state_is_tuple=True)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'output[-1]' means the last output in a an array of outputs. We are using only the last output because we are just classifiying sequences. For example if we feed a sequence to the cell like this \"0100110001\" our network will process them one by one. Every output will affect the next output. We are only interested the final result for each sequence. Below picture is a high level general explanation of the RNN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/RNN-unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn details of LTSM's check out these great posts:\n",
    "\n",
    "[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x):\n",
    "    pred = RNN(x,weights,biases)\n",
    "    output = pred\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    index_of_max_prob = tf.argmax(softmax, 1)\n",
    "    correct_labels =  tf.argmax(y, 1)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 240\n",
    "    display_step = 20\n",
    "    with tf.variable_scope('training'):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Before training|Prediction for first 10 sequences:\",index_of_max_prob.eval({x:testX[:,0:10]}))\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch_index in range(int(n_trainx/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(batch_index)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                if (epoch)%display_step==0 or (epoch+1) == hm_epochs:\n",
    "                    print('Epoch', (epoch), 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                    print('Training accuracy:',accuracy.eval({x:trainX, y:trainY}))\n",
    "                    print('Validation accuracy:',accuracy.eval({x:validX, y:validY}))\n",
    "                    print('Test accuracy:',accuracy.eval({x:testX, y:testY}))\n",
    "                    print(\"--------------------------------------------------------\")\n",
    "            print(\"Optimization finished!\") \n",
    "            print('%50s  %30s' % (\"After training|Prediction for first 10 sequences:\",index_of_max_prob.eval({x:testX[:,0:10]})))\n",
    "            print('%50s  %30s' % (\"Correct labels for first 10 sequences:\",correct_labels.eval({y:testY[:10]})))              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our network and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training|Prediction for first 10 sequences: [0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 0 completed out of 240 loss: 43.5725820065\n",
      "Training accuracy: 0.066\n",
      "Validation accuracy: 0.0475\n",
      "Test accuracy: 0.11\n",
      "--------------------------------------------------------\n",
      "Epoch 20 completed out of 240 loss: 29.8424692154\n",
      "Training accuracy: 0.279333\n",
      "Validation accuracy: 0.29\n",
      "Test accuracy: 0.29\n",
      "--------------------------------------------------------\n",
      "Epoch 40 completed out of 240 loss: 24.6287636757\n",
      "Training accuracy: 0.478\n",
      "Validation accuracy: 0.4575\n",
      "Test accuracy: 0.46\n",
      "--------------------------------------------------------\n",
      "Epoch 60 completed out of 240 loss: 21.5567314625\n",
      "Training accuracy: 0.586667\n",
      "Validation accuracy: 0.5325\n",
      "Test accuracy: 0.57\n",
      "--------------------------------------------------------\n",
      "Epoch 80 completed out of 240 loss: 19.1565725803\n",
      "Training accuracy: 0.691333\n",
      "Validation accuracy: 0.645\n",
      "Test accuracy: 0.63\n",
      "--------------------------------------------------------\n",
      "Epoch 100 completed out of 240 loss: 17.1810283661\n",
      "Training accuracy: 0.774\n",
      "Validation accuracy: 0.73\n",
      "Test accuracy: 0.71\n",
      "--------------------------------------------------------\n",
      "Epoch 120 completed out of 240 loss: 15.5204113722\n",
      "Training accuracy: 0.847333\n",
      "Validation accuracy: 0.7775\n",
      "Test accuracy: 0.76\n",
      "--------------------------------------------------------\n",
      "Epoch 140 completed out of 240 loss: 14.0739548802\n",
      "Training accuracy: 0.896\n",
      "Validation accuracy: 0.8325\n",
      "Test accuracy: 0.78\n",
      "--------------------------------------------------------\n",
      "Epoch 160 completed out of 240 loss: 12.7885556817\n",
      "Training accuracy: 0.921333\n",
      "Validation accuracy: 0.87\n",
      "Test accuracy: 0.87\n",
      "--------------------------------------------------------\n",
      "Epoch 180 completed out of 240 loss: 11.6302398443\n",
      "Training accuracy: 0.942\n",
      "Validation accuracy: 0.9025\n",
      "Test accuracy: 0.9\n",
      "--------------------------------------------------------\n",
      "Epoch 200 completed out of 240 loss: 10.5925425291\n",
      "Training accuracy: 0.954\n",
      "Validation accuracy: 0.9225\n",
      "Test accuracy: 0.91\n",
      "--------------------------------------------------------\n",
      "Epoch 220 completed out of 240 loss: 9.65369355679\n",
      "Training accuracy: 0.963333\n",
      "Validation accuracy: 0.94\n",
      "Test accuracy: 0.92\n",
      "--------------------------------------------------------\n",
      "Epoch 239 completed out of 240 loss: 8.83881998062\n",
      "Training accuracy: 0.972\n",
      "Validation accuracy: 0.9575\n",
      "Test accuracy: 0.93\n",
      "--------------------------------------------------------\n",
      "Optimization finished!\n",
      " After training|Prediction for first 10 sequences:           [5 0 9 4 4 0 6 5 2 9]\n",
      "            Correct labels for first 10 sequences:           [4 0 7 4 4 0 6 5 2 9]\n"
     ]
    }
   ],
   "source": [
    "train(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
