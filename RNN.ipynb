{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow for RNN and Numpy to prepare our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (1024, 10)\n",
      "first element: [ 0.  0.  1.  0.  1.  0.  1.  1.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "num_examples = 2**10\n",
    "seq_length = 10\n",
    "                  \n",
    "sequences = np.random.randint(2, size=(num_examples,seq_length)).astype('float32')\n",
    "\n",
    "print(\"shape of input data:\",sequences.shape)\n",
    "print(\"first element:\", sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify sequences with size 10. For this purpose we're creating a dataset that includes 1024(2^10) example array. Each array consists of 0's and 1's. Number of 1's and 0's is random. Number of 1's determines the array's class. So we can only have 11 classes at most because our array length is 10. (Don't forget we can have all zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 6, ..., 5, 8, 8])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = []\n",
    "for input in sequences: \n",
    "    target = (input == 1).sum()\n",
    "    target_classes.append(target)\n",
    "\n",
    "target_classes = np.asarray(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a supervised learning method we need to know the answers. This means we need to have the correct classes of our sequences for training. So we count the number of 1's for each array and we append them to a 1D array.\n",
    "\n",
    "Now we need to encode our label array with 1-hot encoding. Because in Machine Learning algorithms we tend to encode our class labels with 1-hot encoding. There are a couple of reasons for this. For example, in this problem our network can predict the class labels with probabilities instead of exact class labels. Like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.13317494,  0.01547636,  0.0434029 ,  0.05370335,  0.05800456,\n",
       "        0.01255314,  0.07310165,  0.34522375,  0.07085784,  0.19450151])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.exponential(2,10)\n",
    "sample /= sample.sum()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see why 1-hot encoding is useful in this case. You can think like that:\n",
    "\"With 1-hot encoding we say:\n",
    "\"This is an apple 100% and this is a banana 0%\", instead of saying just \"This is an apple\". Now let's see how can encode our label array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is just creating a unit matrix with the size as a parameter. But if you look carefully this is 1-hot encoded array between 0 and 10. This is actually our 1-hot encoded class labels. We just need to encode our training data using this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = np.eye(11)[target_classes]\n",
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is why Python is awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainX  = sequences[:1000]\n",
    "trainY = target_classes[:1000]\n",
    "testX = sequences[1000:]\n",
    "testY = target_classes[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've splitted our data in two parts. 1.000 of them is for training. And the remaining 24 of them is for testing. We are done with preparing data. Now let's create our RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 128\n",
    "n_chunks = 28\n",
    "x = tf.placeholder(\"float32\", [None, None, seq_length])\n",
    "y = tf.placeholder(\"int32\", [None, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two placeholders. x for input y for class labels. 'n_hidden' means \"number of hidden layers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_hidden, 11]))\n",
    "biases = tf.Variable(tf.random_normal([11]))\n",
    "\n",
    "trainX = trainX.reshape(1,1000,seq_length)\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our weight and bias variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'output[-1]' means the last output in a an array of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x):\n",
    "    pred = RNN(x,weights,biases)\n",
    "    output = pred\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    index_of_max_prob = tf.argmax(softmax, 1)\n",
    "    correct_labels =  tf.argmax(y, 1)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    hm_epochs = 64\n",
    "    with tf.variable_scope('training'):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Before training|Prediction for first 10 sequence:\",index_of_max_prob.eval({x:trainX[0,0:10].reshape(1,10,10)}))\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0\n",
    "                for _ in range(int(num_examples/batch_size)):\n",
    "                    epoch_x, epoch_y = trainX,trainY\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\n",
    "            correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "            print('Accuracy:',accuracy.eval({x:trainX, y:trainY}))\n",
    "            \n",
    "            print(\"After training|Prediction for first 10 sequence:\",index_of_max_prob.eval({x:trainX[0,0:10].reshape(1,10,10)}))\n",
    "            print(\"Correct labels for first 10 sequence\",correct_labels.eval({y:trainY[:10]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our network and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training|Prediction for first 10 sequence: [ 5 10  2  2  2 10 10 10  5 10]\n",
      "Epoch 0 completed out of 64 loss: 23.3513219357\n",
      "Epoch 1 completed out of 64 loss: 19.1296386719\n",
      "Epoch 2 completed out of 64 loss: 18.3281830549\n",
      "Epoch 3 completed out of 64 loss: 17.6780289412\n",
      "Epoch 4 completed out of 64 loss: 16.9752991199\n",
      "Epoch 5 completed out of 64 loss: 16.1118475199\n",
      "Epoch 6 completed out of 64 loss: 15.2795728445\n",
      "Epoch 7 completed out of 64 loss: 14.5651035309\n",
      "Epoch 8 completed out of 64 loss: 13.9359779358\n",
      "Epoch 9 completed out of 64 loss: 13.3870819807\n",
      "Epoch 10 completed out of 64 loss: 12.9158462286\n",
      "Epoch 11 completed out of 64 loss: 12.9056702852\n",
      "Epoch 12 completed out of 64 loss: 12.3442387581\n",
      "Epoch 13 completed out of 64 loss: 11.9593622684\n",
      "Epoch 14 completed out of 64 loss: 11.6067547798\n",
      "Epoch 15 completed out of 64 loss: 11.3422005177\n",
      "Epoch 16 completed out of 64 loss: 11.0622347593\n",
      "Epoch 17 completed out of 64 loss: 10.7403671741\n",
      "Epoch 18 completed out of 64 loss: 10.4333821535\n",
      "Epoch 19 completed out of 64 loss: 10.4407304525\n",
      "Epoch 20 completed out of 64 loss: 10.2468949556\n",
      "Epoch 21 completed out of 64 loss: 9.82564121485\n",
      "Epoch 22 completed out of 64 loss: 9.5451875329\n",
      "Epoch 23 completed out of 64 loss: 9.30706256628\n",
      "Epoch 24 completed out of 64 loss: 9.11092239618\n",
      "Epoch 25 completed out of 64 loss: 8.88668555021\n",
      "Epoch 26 completed out of 64 loss: 8.70939064026\n",
      "Epoch 27 completed out of 64 loss: 8.56203901768\n",
      "Epoch 28 completed out of 64 loss: 8.48545330763\n",
      "Epoch 29 completed out of 64 loss: 8.14189642668\n",
      "Epoch 30 completed out of 64 loss: 7.89383298159\n",
      "Epoch 31 completed out of 64 loss: 7.69589948654\n",
      "Epoch 32 completed out of 64 loss: 7.77029538155\n",
      "Epoch 33 completed out of 64 loss: 7.40143823624\n",
      "Epoch 34 completed out of 64 loss: 7.11860352755\n",
      "Epoch 35 completed out of 64 loss: 6.88126623631\n",
      "Epoch 36 completed out of 64 loss: 6.79685300589\n",
      "Epoch 37 completed out of 64 loss: 6.47324293852\n",
      "Epoch 38 completed out of 64 loss: 6.24822211266\n",
      "Epoch 39 completed out of 64 loss: 6.38828015327\n",
      "Epoch 40 completed out of 64 loss: 6.08373034\n",
      "Epoch 41 completed out of 64 loss: 5.84027886391\n",
      "Epoch 42 completed out of 64 loss: 5.55712485313\n",
      "Epoch 43 completed out of 64 loss: 5.30375623703\n",
      "Epoch 44 completed out of 64 loss: 5.1105607748\n",
      "Epoch 45 completed out of 64 loss: 5.08032700419\n",
      "Epoch 46 completed out of 64 loss: 5.20123422146\n",
      "Epoch 47 completed out of 64 loss: 4.73118183017\n",
      "Epoch 48 completed out of 64 loss: 4.43988367915\n",
      "Epoch 49 completed out of 64 loss: 4.21490707994\n",
      "Epoch 50 completed out of 64 loss: 4.12856006622\n",
      "Epoch 51 completed out of 64 loss: 5.29585543275\n",
      "Epoch 52 completed out of 64 loss: 4.58914461732\n",
      "Epoch 53 completed out of 64 loss: 4.06293442845\n",
      "Epoch 54 completed out of 64 loss: 3.76065099239\n",
      "Epoch 55 completed out of 64 loss: 3.54337266088\n",
      "Epoch 56 completed out of 64 loss: 3.36900019646\n",
      "Epoch 57 completed out of 64 loss: 3.29197156429\n",
      "Epoch 58 completed out of 64 loss: 3.14627611637\n",
      "Epoch 59 completed out of 64 loss: 3.07974439859\n",
      "Epoch 60 completed out of 64 loss: 3.06515234709\n",
      "Epoch 61 completed out of 64 loss: 2.88359779119\n",
      "Epoch 62 completed out of 64 loss: 2.71227538586\n",
      "Epoch 63 completed out of 64 loss: 2.65728032589\n",
      "Accuracy: 0.96\n",
      "After training|Prediction for first 10 sequence: [6 6 6 6 4 3 5 6 4 7]\n",
      "Correct labels for first 10 sequence [5 4 6 6 4 3 5 6 4 7]\n"
     ]
    }
   ],
   "source": [
    "train(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
