{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use TensorFlow for RNN and Numpy to prepare our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input data: (3000, 10)\n",
      "first element: [ 0.  0.  0.  1.  1.  0.  1.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "num_examples = 3000\n",
    "seq_length = 10\n",
    "sequences = np.empty((num_examples,seq_length))\n",
    "for i in range(num_examples):\n",
    "    seq = np.random.randint(2, size=(seq_length)).astype('float32')\n",
    "    sequences[i] = seq\n",
    "\n",
    "print(\"shape of input data:\",sequences.shape)\n",
    "print(\"first element:\", sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective is to classify sequences with size 10. For this purpose we're creating a dataset that includes 3.000 example array. Each array consists of 0's and 1's. Number of 1's and 0's is random. Number of 1's determines the array's class. So we can only have 11 classes at most because our array length is 10. (Don't forget we can have all zeroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 7, ..., 3, 5, 5])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = []\n",
    "for input in sequences: \n",
    "    target = (input == 1).sum()\n",
    "    target_classes.append(target)\n",
    "\n",
    "target_classes = np.asarray(target_classes)\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a supervised learning method we need to know the answers. This means we need to have the correct classes of our sequences for training. So we count the number of 1's for each array and we append them to a 1D array.\n",
    "\n",
    "Now we need to encode our label array with 1-hot encoding. Because in Machine Learning algorithms we tend to encode our class labels with 1-hot encoding. There are a couple of reasons for this. For example, in this problem our network can predict the class labels with probabilities instead of exact class labels. Like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10612717,  0.06763875,  0.06630916,  0.07286086,  0.18861263,\n",
       "        0.03540382,  0.24893265,  0.12198714,  0.0880622 ,  0.00406562])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = np.random.exponential(2,10)\n",
    "sample /= sample.sum()\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think you can see why 1-hot encoding is useful in this case. You can think like that:\n",
    "\"With 1-hot encoding we say:\n",
    "\"This is an apple 100% and this is a banana 0%\", instead of saying just \"This is an apple\". Now let's see how we can encode our label array.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is just creating a unit matrix with the size as a parameter. But if you look carefully this is 1-hot encoded array between 0 and 10. This is actually our 1-hot encoded class labels. We just need to encode our training data using this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes = np.eye(11)[target_classes]\n",
    "target_classes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! This is why Python is awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2000, 10), (1, 900, 10), (1, 100, 10), (2000, 11), (900, 11), (100, 11))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_training = 2000\n",
    "trainX  = sequences[:n_training]\n",
    "trainY = target_classes[:n_training]\n",
    "validX = sequences[n_training:2900]\n",
    "validY = target_classes[n_training:2900]\n",
    "testX = sequences[2900:]\n",
    "testY = target_classes[2900:]\n",
    "\n",
    "trainX = trainX.reshape(1,trainX.shape[0],trainX.shape[1])\n",
    "validX = validX.reshape(1,validX.shape[0],validX.shape[1])\n",
    "testX = testX.reshape(1,testX.shape[0],testX.shape[1])\n",
    "\n",
    "trainX.shape,validX.shape,testX.shape,trainY.shape,validY.shape,testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've splitted our data in two parts. 2.000 of them is for training, 900 for validation and remaining 100 is for testing. We've reshaped our input data in a 3D shape. Because that's what TensorFlow RNN function's requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_hidden = 128\n",
    "n_chunks = 28\n",
    "x = tf.placeholder(\"float32\", [None, None, seq_length])\n",
    "y = tf.placeholder(\"int32\", [None, 11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(batch_index):\n",
    "        # Go to the next epoch\n",
    "        if batch_index*batch_size + batch_size > trainX.shape[1]:\n",
    "            # Finished epoch\n",
    "            # Get the rest examples in this epoch\n",
    "            rest_num_examples = trainX.shape[1] - batch_index*batch_size\n",
    "            input_rest_part = trainX[:,trainX.shape[1]-rest_num_examples:trainX.shape[1]]\n",
    "            labels_rest_part = trainY[trainY.shape[0]-rest_num_examples:trainY.shape[0]]\n",
    "            # Start next epoch\n",
    "            batch_index = 0\n",
    "            start = batch_index * batch_size\n",
    "            end = start + batch_size\n",
    "            input_new_part = trainX[:,start:end]\n",
    "            labels_new_part = trainY[start:end]\n",
    "            batch_index += 1\n",
    "            return np.concatenate((input_rest_part, input_new_part), axis=1), np.concatenate(\n",
    "                (labels_rest_part, labels_new_part), axis=0)\n",
    "        else:\n",
    "            start = batch_index*batch_size\n",
    "            end = start+batch_size\n",
    "            batch_index += 1\n",
    "            return trainX[:,start:end], trainY[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two placeholders. x for input y for class labels. 'n_hidden' means \"number of hidden layers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([n_hidden, 11]))\n",
    "biases = tf.Variable(tf.random_normal([11]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 10)\n",
      "(100, 11)\n",
      "(1, 100, 10)\n",
      "(100, 11)\n",
      "(1, 100, 10)\n",
      "(100, 11)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    ex,ey = next_batch(i)\n",
    "    print(ex.shape)\n",
    "    print(ey.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've defined our weight and bias variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code splits the training data to batches in batch_size according to the epoch number that provided as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "    \n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = tf.nn.dynamic_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights) + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'output[-1]' means the last output in a an array of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x):\n",
    "    pred = RNN(x,weights,biases)\n",
    "    output = pred\n",
    "    softmax = tf.nn.softmax(output)\n",
    "    index_of_max_prob = tf.argmax(softmax, 1)\n",
    "    correct_labels =  tf.argmax(y, 1)\n",
    "    \n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y) )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "    \n",
    "    hm_epochs = 200\n",
    "    display_step = 20\n",
    "    with tf.variable_scope('training'):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Before training|Prediction for first 10 sequence:\",index_of_max_prob.eval({x:testX[:,0:10]}))\n",
    "            for epoch in range(hm_epochs):\n",
    "                epoch_loss = 0\n",
    "                for batch_index in range(int(n_training/batch_size)):\n",
    "                    epoch_x, epoch_y = next_batch(batch_index)\n",
    "                    _, c = sess.run([optimizer, cost], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                    epoch_loss += c\n",
    "\n",
    "                if (epoch)%display_step==0:\n",
    "                    print('Epoch', epoch, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "                    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "                    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "                    if(epoch==hm_epochs-1):\n",
    "                        print(\"Optimization finished!\")\n",
    "                    print('Training accuracy:',accuracy.eval({x:trainX, y:trainY}))\n",
    "                    print('Validation accuracy:',accuracy.eval({x:validX, y:validY}))\n",
    "                    print('Test accuracy:',accuracy.eval({x:testX, y:testY}))\n",
    "                    print(\"--------------------------------------------------------\")\n",
    "                  \n",
    "            print(\"After training|Prediction for first 10 sequence:\",index_of_max_prob.eval({x:testX[:,0:10]}))\n",
    "            print(\"Correct labels for first 10 sequence\",correct_labels.eval({y:testY[:10]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train our network and make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training|Prediction for first 10 sequence: [9 1 1 1 9 9 9 9 9 9]\n",
      "Epoch 0 completed out of 200 loss: 67.9889593124\n",
      "Training accuracy: 0.2035\n",
      "Validation accuracy: 0.188889\n",
      "Test accuracy: 0.18\n",
      "--------------------------------------------------------\n",
      "Epoch 20 completed out of 200 loss: 35.622640729\n",
      "Training accuracy: 0.3035\n",
      "Validation accuracy: 0.281111\n",
      "Test accuracy: 0.27\n",
      "--------------------------------------------------------\n",
      "Epoch 40 completed out of 200 loss: 30.9267612696\n",
      "Training accuracy: 0.3575\n",
      "Validation accuracy: 0.343333\n",
      "Test accuracy: 0.32\n",
      "--------------------------------------------------------\n",
      "Epoch 60 completed out of 200 loss: 28.0669082403\n",
      "Training accuracy: 0.4235\n",
      "Validation accuracy: 0.39\n",
      "Test accuracy: 0.39\n",
      "--------------------------------------------------------\n",
      "Epoch 80 completed out of 200 loss: 26.2641035318\n",
      "Training accuracy: 0.4785\n",
      "Validation accuracy: 0.448889\n",
      "Test accuracy: 0.42\n",
      "--------------------------------------------------------\n",
      "Epoch 100 completed out of 200 loss: 24.8646485806\n",
      "Training accuracy: 0.554\n",
      "Validation accuracy: 0.514444\n",
      "Test accuracy: 0.53\n",
      "--------------------------------------------------------\n",
      "Epoch 120 completed out of 200 loss: 23.6521703005\n",
      "Training accuracy: 0.623\n",
      "Validation accuracy: 0.582222\n",
      "Test accuracy: 0.63\n",
      "--------------------------------------------------------\n",
      "Epoch 140 completed out of 200 loss: 22.5931765437\n",
      "Training accuracy: 0.66\n",
      "Validation accuracy: 0.625556\n",
      "Test accuracy: 0.65\n",
      "--------------------------------------------------------\n",
      "Epoch 160 completed out of 200 loss: 21.6512281895\n",
      "Training accuracy: 0.6805\n",
      "Validation accuracy: 0.647778\n",
      "Test accuracy: 0.66\n",
      "--------------------------------------------------------\n",
      "Epoch 180 completed out of 200 loss: 20.793251574\n",
      "Training accuracy: 0.705\n",
      "Validation accuracy: 0.672222\n",
      "Test accuracy: 0.69\n",
      "--------------------------------------------------------\n",
      "After training|Prediction for first 10 sequence: [7 6 5 6 5 5 5 3 6 6]\n",
      "Correct labels for first 10 sequence [6 4 3 6 5 5 5 2 6 7]\n"
     ]
    }
   ],
   "source": [
    "train(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
